{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "\n",
    "Apache Spark with Python in Jupyter Notebooks.\n",
    "\n",
    "I have `spark-shell` running with Scala, and have set environment variables in Bash under which my `jupyter notebook` is running.\n",
    "\n",
    "    export PYSPARK_PYTHON=python3\n",
    "    export PYSPARK_DRIVER_PYTHON=ipython\n",
    "    export PYSPARK_DRIVER_PYTHON_OPTS='notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/spark/python')\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to `local`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(master='local', appName=\"notebooklearning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is, did we just spawn a new Spark, or connect to what we have running?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data stuff. We use RDD which is the Spark thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([('a',7),('a',2),('b',2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, an operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 7, 'a', 2, 'b', 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok that ran, but new SparkUIs were made at http://raspberrypi:4041 and http://raspberrypi:4042."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to `spark://` in standalone mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I instead have `/opt/spark/sbin/start-master.sh` and `/opt/spark/sbin/start-master.sh` running. Getting connection refused until I restarted the kernel, after the above. Ok so `sc.stop()` left something in my Python kernel. I was killing stuff from command line though, and had a number of spark Context open so I am not surprised. Aaaanyway http://raspberrypi:8080 and detail UI at http://raspberrypi:4040."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkconf = pyspark.SparkConf().setAppName('notebookLearning2').setMaster('spark://raspberrypi:7077')\n",
    "sc2 = pyspark.SparkContext(conf=sparkconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc2.parallelize([('a',7),('a',2),('b',2)])\n",
    "rdd2 = sc2.parallelize([(\"a\",[\"x\",\"y\",\"z\"]), (\"b\",[\"p\", \"r\"])])\n",
    "rdd3 = sc2.parallelize(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 7, 'a', 2, 'b', 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.flatMapValues(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple access to (key, value) tuples, which I have in `rdd1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'b']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.values().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 100, mean: 49.5, stdev: 28.8660700477, max: 99.0, min: 0.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can run the Scala `spark-shell`... or could, if I didn't run out of memory. Little RaspberryPi 3 is busy!\n",
    "\n",
    "To constrain memory usage, I added the following four lines, without knowing what each of them exactly means, to `conf/spark-env.sh`.\n",
    "\n",
    "    SPARK_EXECUTOR_MEMORY=500m\n",
    "    SPARK_DRIVER_MEMORY=500m\n",
    "    SPARK_WORKER_MEMORY=500m\n",
    "    SPARK_DAEMON_MEMORY=500m\n",
    "\n",
    "I think I might prefer to do such settings in `conf/spakr-default.conf` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in Scala, I now do\n",
    "\n",
    "    val inputfile = sc.textFile(\"input.txt\")\n",
    "    val counts = inputfile.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey(_+_);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
